{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №3\n",
    "\n",
    "**Задание №1**\n",
    "\n",
    "Скачайте текст \"Литературных анекдотов\". Напишите функцию, которая будет **читать файл, лемматизировать текст с помощью pymystem3 и записывать результат в новый файл**. У функции должно бы два аргумента: путь к исходному файлу и путь к файлу с лемматизированным текстом. Вызов функции тоже должен быть прописан в решении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2763\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem #устанавливаем библиотеку mystem\n",
    "def newfunction (read_file, new_file):\n",
    "    f = open (read_file, 'r', encoding='utf-8') #откроем файл \n",
    "    text = f.read() #прочитаем его, чтобы данные из него загрузить в память\n",
    "    punctuation= [word.strip(\"!?.,:;«»-\") for word in text.split()] #обрежем пунктуацию\n",
    "    new=' '.join(punctuation)\n",
    "    m = Mystem()\n",
    "    lemmas = m.lemmatize(new) #лемматизируем слова в тексте\n",
    "    text_for_file = ''.join(lemmas) #объединим через пробел получившийся список\n",
    "    with open(new_file, 'w', encoding='utf-8') as d: #откроем новый файл для записи \n",
    "        d.write(text_for_file) #записываем лемматизированный текст в новый файл\n",
    "    return text_for_file \n",
    "final_text= newfunction ('literary.anecdotes.txt', 'new_file.txt')\n",
    "all_words = len(final_text.split()) \n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание № 2**\n",
    "\n",
    "**Очистите лемматизированный текст от стоп-слов** и посчитайте **ipm** для оставшихся. **Выведите 20 самых частотных** по этому параметру слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('пушкин', 19182.04849800941), ('гоголь', 10857.763300760043), ('толстой', 10857.763300760043), ('однажды', 10495.837857401375), ('лев', 9048.136083966703), ('любить', 6876.583423814694), ('достоевский', 6514.657980456026), ('тургенев', 5428.881650380022), ('царствие', 5428.881650380022), ('небесный', 5428.881650380022), ('ребенок', 5066.956207021353), ('окно', 4343.105320304017), ('тверской', 4343.105320304017), ('бульвар', 4343.105320304017), ('приходить', 3981.1798769453494), ('федор', 3981.1798769453494), ('михайлович', 3981.1798769453494), ('лермонтов', 3619.254433586681), ('идти', 3619.254433586681), ('герцен', 3619.254433586681)]\n"
     ]
    }
   ],
   "source": [
    "with open('rus_stopwords.txt', 'r', encoding='utf-8') as q: #открываем имеющийся файл со стоп-словами)\n",
    "    stop_words = q.read().split('\\n') \n",
    "    without_stop_words = [w for w in final_text.split() if w not in stop_words] #убираем стоп-слова из лемматизированного текста \n",
    "    clean_text = ' '.join(without_stop_words) #новый текст\n",
    "from collections import Counter\n",
    "counts = Counter(clean_text.split()) \n",
    "quantity = {} #создадим новый словарь, где ключ останется тем же, слово, а значение - ipm, абсолютная частота/ длину текста \n",
    "for value, key in counts.items():\n",
    "    ipm = key/all_words * 1000000 \n",
    "    quantity[value] = ipm\n",
    "new_counts = Counter(quantity)\n",
    "print(new_counts.most_common(20)) #выводим 20 самых частотных слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание №3**\n",
    "\n",
    "Сделайте полный **морфологический разбор** исходного текста. **Напишите регулярное выражение**, которое будет извлекать из тега только часть речи. Пройдитесь циклом по списку с разборами, который выдал pymystem3, извлекая из каждого разбора форму слова и его часть речи и **записывая их в новый словарь (форма -- ключ, часть речи -- значение)**. Посчитайте **абсолютную частоту** для всех частей речи, а затем **относительнную частоту** (абсолютная частота / длина текста в словах)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2763\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "with open('literary.anecdotes.txt', 'r', encoding='utf-8') as f:\n",
    "    file = f.read()\n",
    "    punctuation= [word.strip(\"!?.,:;«»-\") for word in file.split()] #обрежем пунктуацию\n",
    "    new=' '.join(punctuation) #объединим в список через пробел\n",
    "import json  \n",
    "text = m.analyze(new)\n",
    "grammar = text[0]['analysis'][0]['gr'] # достаем только грамматический разбор\n",
    "with open('text.json','w', encoding='utf-8') as f: #запишем полученную информацию\n",
    "    json.dump(text, f, ensure_ascii=False) # переводим в строку\n",
    "with open('text.json', 'r', encoding='utf-8') as f:\n",
    "    text = json.loads(f.read()) \n",
    "words = {} #создадим пустой словарь\n",
    "for word in text:\n",
    "    try:\n",
    "        form = word['text'] #где form - ключ\n",
    "        grammar = word['analysis'][0]['gr'] #берем грамматический разбор\n",
    "        words[form]=grammar\n",
    "    except (KeyError, IndexError) as e: #исключаем все виды ошибок\n",
    "        pass\n",
    "import re \n",
    "part_of_speech = re.compile('[A-Z]+') # берем заглавные буквы, использующиеся 1 и более раз\n",
    "words_part= {}\n",
    "for key, value in words.items():  #проходимся циклом по разбору pymystem 3, используя items, чтобы работать с парой ключ-значение\n",
    "        words_part[key] = part_of_speech.match(value).group(0)  #используем метод match и group\n",
    "frequency = new.split()\n",
    "print(len(frequency)) #посчитаем абсолютную частоту для всех частей речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('S', 159971.04596453128), ('V', 157437.56786102062), ('A', 35830.618892508144), ('ADV', 31125.58812884546), ('SPRO', 19543.973941368076), ('APRO', 18096.272167933406), ('PR', 13391.24140427072), ('PART', 10133.912414042707), ('ADVPRO', 8686.210640608035), ('CONJ', 7962.359753890699), ('NUM', 3257.328990228013), ('INTJ', 2533.4781035106766), ('ANUM', 723.8508867173363)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "pos=' '.join([f'{key}: {value}' for key, value in words_part.items()]) #переведем словарь в строку\n",
    "new= Counter(pos.split()) \n",
    "pos_frequency = {} #создадим новый словарь, пройдемся по нему циклом, чтобы посчитать относительную частоту для всех частей речи\n",
    "for value, key in new.items():\n",
    "    ipm = key / len(frequency) *1000000\n",
    "    pos_frequency[value] = ipm\n",
    "newcounts = Counter(pos_frequency)\n",
    "print(newcounts.most_common(13)) #выводим относительную частоту для всех частей речи\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
