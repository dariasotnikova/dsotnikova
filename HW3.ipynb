{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №3\n",
    "\n",
    "**Задание №1**\n",
    "\n",
    "Скачайте текст \"Литературных анекдотов\". Напишите функцию, которая будет **читать файл, лемматизировать текст с помощью pymystem3 и записывать результат в новый файл**. У функции должно бы два аргумента: путь к исходному файлу и путь к файлу с лемматизированным текстом. Вызов функции тоже должен быть прописан в решении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open ('literary.anecdotes.txt', 'r', encoding='utf-8') #откроем файл \n",
    "text = f.read() #прочитаем его, чтобы данные из него загрузить в память\n",
    "punctuation= [word.strip(\"!?.,:;«»-\") for word in text.split()] #обрежем пунктуацию\n",
    "new=' '.join(punctuation)\n",
    "punctuation2= [w for w in new.split() if w.isalnum()] #обрежем пунктуацию с генератором списков. В тексте останутся только буквы алфавита и цифры\n",
    "new_text=' '.join(punctuation2) #объединим в строку слова из получившегося списка через пробел\n",
    "from pymystem3 import Mystem #устанавливаем библиотеку mystem\n",
    "m = Mystem()\n",
    "lemmas = m.lemmatize(new_text) #лемматизируем слова в тексте\n",
    "type(lemmas) # функция возвратила список\n",
    "text_for_file = ''.join(lemmas) #объединим через пробел получившийся список\n",
    "with open('lemmas.txt', 'w', encoding='utf-8') as d: #откроем новый файл для записи \n",
    "         d.write(text_for_file) #записываем лемматизированный текст в новый файл"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание № 2**\n",
    "\n",
    "**Очистите лемматизированный текст от стоп-слов** и посчитайте ipm для оставшихся. **Выведите 20 самых частотных** по этому параметру слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16005\n",
      "[('пушкин', 53), ('гоголь', 30), ('толстой', 30), ('однажды', 29), ('лев', 25), ('любить', 19), ('достоевский', 18), ('тургенев', 15), ('царствие', 15), ('небесный', 15), ('ребенок', 14), ('окно', 12), ('тверской', 12), ('бульвар', 12), ('приходить', 11), ('федор', 11), ('михайлович', 11), ('лермонтов', 10), ('идти', 10), ('герцен', 10)]\n"
     ]
    }
   ],
   "source": [
    "f = open ('lemmas.txt', 'r', encoding ='utf-8') #открываем файл с лемматизированным текстом\n",
    "text = f.read() #читаем его\n",
    "quantity = len(text) #длина текста\n",
    "print(quantity)\n",
    "d = open('rus_stopwords.txt', 'r', encoding='utf-8') #открываем имеющийся файл со стоп-словами\n",
    "stop_words = d.read().split('\\n') \n",
    "without_stop_words = [w for w in text.split() if w not in stop_words] #убираем стоп-слова из лемматизированного текста \n",
    "clean_text = ' '.join(without_stop_words) #новый текст\n",
    "from collections import Counter\n",
    "new_counts = Counter(without_stop_words)\n",
    "print(new_counts.most_common(20)) #выводим 20 самых частотных слов\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание №3**\n",
    "\n",
    "Сделайте полный морфологический разбор исходного текста. Напишите регулярное выражение, которое будет извлекать из тега только часть речи. Пройдитесь циклом по списку с разборами, который выдал pymystem3, извлекая из каждого разбора форму слова и его часть речи и записывая их в новый словарь (форма -- ключ, часть речи -- значение). Посчитайте абсолютную частоту для всех частей речи, а затем относительнную частоту (абсолютная частота / длина текста в словах)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "with open('literary.anecdotes.txt', 'r', encoding='utf-8') as f:\n",
    "    file = f.read()\n",
    "    punctuation= [word.strip(\"!?.,:;«»-\") for word in file.split()] #обрежем пунктуацию\n",
    "    new=' '.join(punctuation) #объединим в список через пробел\n",
    "    punctuation2= [w for w in new.split() if w.isalnum()] #обрежем пунктуацию, останутся только буквы алфавита и цифры\n",
    "    new_text=' '.join(punctuation2) # объединим полученный список в строку\n",
    "import json  \n",
    "text = m.analyze(new_text)\n",
    "grammar = text[0]['analysis'][0]['gr'] # достаем только грамматический разбор\n",
    "with open('text.json','w', encoding='utf-8') as f: #запишем полученную информацию\n",
    "    json.dump(text, f, ensure_ascii=False) # переводим в строку\n",
    "with open('text.json', 'r', encoding='utf-8') as f:\n",
    "    text = json.loads(f.read()) \n",
    "words = {} #создадим пустой словарь\n",
    "for word in text:\n",
    "    try:\n",
    "        form = word['text'] #где form - ключ\n",
    "        grammar = word['analysis'][0]['gr'] #берем грамматический разбор\n",
    "        words[form]=grammar\n",
    "    except (KeyError, IndexError) as e: #исключаем все виды ошибок\n",
    "        pass\n",
    "import re \n",
    "part_of_speech = re.compile('[A-Z]+') # берем заглавные буквы, использующиеся 1 и более раз\n",
    "words_part= {}\n",
    "for key, value in words.items():  #проходимся циклом по разбору pymystem 3, используя items, чтобы работать с парой ключ-значение\n",
    "        words_part[key] = part_of_speech.match(value).group(0)  #используем метод match и group"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
